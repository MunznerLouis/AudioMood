{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation 1\n",
    "This first version is used to provide the data to the model as raw as possible, without any adjustments or parameters set. The goal of this initial version is to see the \"worst\" possible model that can be created with this data. By doing so, the model can help identify the weaknesses in the data, and highlight areas where more attention and data collection may be needed. The idea behind this approach is that the subsequent versions of the model will only do better by adjusting lots of parameters and our way of preparing the data. As the model is optimized with more parameters, it should become more accurate and effective in predicting emotions. In essence, the first version serves as a benchmark for the model, showing how far it has come from its initial state and providing insight into the progress that has been made over time.\n",
    "\n",
    "Initially, we extract the data into four separate lists. The initial list contains the audio waves, which are then split into chunks of 2 seconds to facilitate feeding it to the AI. \n",
    "The remaining three lists consist of labels, which appear to have three distinct interpretations. However, at present, we will only concentrate on using the first list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  #MAKE SURE TO IMPORT MATPLOTLIB BEFORE LIBROSA, otherwise matplolib will return errors somehow..\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "label_dir = \"../labels/\"\n",
    "audio_dir = \"../audio/\"\n",
    "\n",
    "all_labels_version1 = []\n",
    "all_labels_version2 = []\n",
    "all_labels_version3 = []\n",
    "all_chunks = []\n",
    "\n",
    "all_audios = []  #only here for stats\n",
    "\n",
    "#Data processing - audio chunking - labels onehot encoding\n",
    "\n",
    "# iterate over the files in the directory, grouped by three\n",
    "for i in range(0, len(os.listdir(label_dir)), 3):\n",
    "    if os.listdir(label_dir)[i].endswith('.txt'):\n",
    "        tmp_emotion1 =[]\n",
    "        tmp_emotion2 =[]\n",
    "        tmp_emotion3 =[]\n",
    "        tmp_wav_file = []\n",
    "        # read the content of each file\n",
    "        with open(os.path.join(label_dir, os.listdir(label_dir)[i])) as f1:\n",
    "            for line in f1:\n",
    "                tmp_emotion1.append(re.search(r':(\\w+)', line)[1])\n",
    "                tmp_wav_file.append(re.search(r'(\\w+)',line)[1])\n",
    "            \n",
    "        with open(os.path.join(label_dir, os.listdir(label_dir)[i+1])) as f2:\n",
    "            for line in f2:\n",
    "                tmp_emotion2.append(re.search(r':(\\w+)', line)[1])\n",
    "        \n",
    "        with open(os.path.join(label_dir, os.listdir(label_dir)[i+2])) as f3:\n",
    "            for line in f3:\n",
    "                tmp_emotion3.append(re.search(r':(\\w+)', line)[1])\n",
    "\n",
    "            #exemple tmp_wav_file[i] -> Ses01F_impro02_F005 :Sadness; ()\n",
    "            #example2                -> Ses01F_script01_1_F000 :Fear; ()\n",
    "            for i in range(len(tmp_wav_file)):\n",
    "                first_matching_file = None\n",
    "                reg_session = re.search(r'Ses(\\d+)', tmp_wav_file[i])[1]\n",
    "                reg_impro = re.search(r'\\d{2}([A-Za-z].*?_[A-Za-z].*?)_[A-Z]', tmp_wav_file[i]).group(1)\n",
    "                audio_file = os.path.join(f\"../audio/Session{reg_session[1]}/sentences/wav/Ses{reg_session}{reg_impro}/\", tmp_wav_file[i]+\".wav\")\n",
    "\n",
    "                y, sr = librosa.load(audio_file, sr=None, mono=True)\n",
    "                y = librosa.to_mono(y)\n",
    "\n",
    "                all_audios.append(y)\n",
    "\n",
    "                # Calculate the duration of each chunk in samples\n",
    "                chunk_length_samples = sr * 2\n",
    "\n",
    "                # Split the audio file into chunks\n",
    "                chunks = len(y) // chunk_length_samples\n",
    "                if len(y)/chunk_length_samples<1.25:\n",
    "                    all_chunks.append(y)\n",
    "                    all_labels_version1.append(tmp_emotion1[i])\n",
    "                    all_labels_version2.append(tmp_emotion2[i])\n",
    "                    all_labels_version3.append(tmp_emotion3[i])\n",
    "\n",
    "                for i in range(chunks):\n",
    "                    start = i * chunk_length_samples\n",
    "                    end = (i + 1) * chunk_length_samples\n",
    "                    if end > len(y):\n",
    "                        end = len(y)\n",
    "                    chunk = y[start:end]\n",
    "                    # Export each chunk to a new audio file\n",
    "                    all_chunks.append(chunk)\n",
    "                    all_labels_version1.append(tmp_emotion1[i])\n",
    "                    all_labels_version2.append(tmp_emotion2[i])\n",
    "                    all_labels_version3.append(tmp_emotion3[i])\n",
    "\n",
    "                print(audio_file , tmp_emotion1[i],tmp_emotion2[i],tmp_emotion3[i],end='\\n   ')\n",
    "                print(f\"{round(len(y)/sr,2)}s divis√© en {chunks}\")\n",
    "print(\"audio chunking finished\")\n",
    "encoder = OneHotEncoder()\n",
    "encoded_labels1 = encoder.fit_transform([[label] for label in all_labels_version1]).toarray()\n",
    "encoded_labels2 = encoder.fit_transform([[label] for label in all_labels_version2]).toarray()\n",
    "encoded_labels3 = encoder.fit_transform([[label] for label in all_labels_version3]).toarray()\n",
    "print(\"finished !\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then add padding to the audio waves since they don't have exactly the same length as we can see on this plot :\n",
    "\n",
    "<img src=\"plots/line_plot_chunked_length_sorted.png\" alt=\"line plot chunked length sorted\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data padding \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# find the maximum length of the soundwaves\n",
    "max_length = max([len(soundwave) for soundwave in all_chunks])\n",
    "\n",
    "# apply padding of zeros\n",
    "padded_soundwaves = np.array([np.pad(soundwave, (0, max_length - len(soundwave)), mode='constant') for soundwave in all_chunks], dtype='float32')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data in - final_dataset/\n",
    "import numpy as np\n",
    "\n",
    "np.save(\"processing_dataset/padded_soundwaves.npy\", padded_soundwaves)\n",
    "print(\"Padded_soundwaves saved!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates MEL-spectograms for each elements of padded_soundwaves\n",
    "\n",
    "sr = 22050\n",
    "n_fft = 2048\n",
    "mel_spec_array=[]\n",
    "# generate MEL spectrograms for each padded soundwave\n",
    "for soundwave in padded_soundwaves:\n",
    "    mel_spec = librosa.feature.melspectrogram(y=soundwave, sr=sr, n_fft=n_fft, hop_length=int(n_fft/2))\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # convert to decibel scale\n",
    "    mel_spec_array.append(mel_spec_db)\n",
    "    # plot the MEL spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mel_spec_db, sr=sr, hop_length=int(n_fft/2), x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('MEL Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data in - final_dataset/\n",
    "import numpy as np\n",
    "\n",
    "mel_spec_array = np.array(mel_spec_array)\n",
    "np.save(\"final_dataset/spectrograms.npy\", mel_spec_array)\n",
    "print(\"spectrograms saved!\") \n",
    "\n",
    "np.savez('final_dataset/labels.npz', labels1=encoded_labels1, labels2=encoded_labels2, labels3=encoded_labels3)\n",
    "print(\"labels saved!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "data = np.load('final_dataset/labels.npz')\n",
    "# Extract the onehot encoded labels\n",
    "labels1 = data['labels1']\n",
    "labels2 = data['labels2']\n",
    "labels3 = data['labels3']\n",
    "print(\"labels load finished\\n\",labels1[:5])\n",
    "\n",
    "spectrograms = np.load(\"final_dataset/spectrograms.npy\")\n",
    "print(\"Spectrograms load finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This script deletes all the .anvil files in a specified directory since those are useless for our use in this project. \n",
    "\n",
    "import glob\n",
    "# Directory containing the audio files\n",
    "audio_dir = \"labels/\"\n",
    "\n",
    "# List all the .anvil files in the directory\n",
    "anvil_files = glob.glob(os.path.join(audio_dir, \"*.anvil\"))\n",
    "\n",
    "# Loop over all the .anvil files and delete them\n",
    "for file_path in anvil_files:\n",
    "    os.remove(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
