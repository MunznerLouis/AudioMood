{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAVDESS Data Preparation Model 2\n",
    "\n",
    "## Choice of Feature Representation\n",
    "\n",
    "For this model, I have decided not to use mel-spectrograms as a feature representation anymore. Instead, we will explore the use of MFCCs (Mel Frequency Cepstral Coefficients) as they might be more applicable for machine learning and classification tasks.\n",
    "\n",
    "MFCCs are a variant of mel-spectrograms that capture the spectral characteristics of audio signals. They provide a compact representation of the audio data by extracting relevant features such as pitch, timbre, and texture.\n",
    "\n",
    "By leveraging MFCCs, I aim to improve the performance of my machine learning models since my goal is to reach 80% (and beyond !).\n",
    "\n",
    "## Processing MFCCs\n",
    "\n",
    "In contrast to our previous approach of directly feeding the 2D image of Mel-spectrogram, we will employ a different method for processing the MFCCs in this model.\n",
    "\n",
    "Specifically, we will compute the mean across the time axis of the MFCCs. This approach serves two purposes: dimension reduction and noise reduction. By taking the mean, we transform the 2D MFCC representation into a 1D feature vector, simplifying the input for the model. Additionally, averaging the MFCCs helps mitigate the impact of short-term variations and noise.\n",
    "\n",
    "While this aggregation technique may result in the loss of some audio information, it can be seen as an opportunity for the model to generalize better. To account for this, we may consider increasing the number of epochs during training. Fortunately with this new approach, the dataset we are working with is lightweight compared to previous models, so this adjustment should not place a significant burden on computational resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  #MAKE SURE TO IMPORT MATPLOTLIB BEFORE LIBROSA, otherwise matplolib will return errors somehow..\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "audio_dir = \"../../RAVDESS_dataset/\"\n",
    "\n",
    "\"\"\" \n",
    "Modality            (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "Vocal channel       (01 = speech, 02 = song).\n",
    "Emotion             (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "Emotional intensity (01 = normal, 02 = strong).     \n",
    "Statement           (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "Repetition          (01 = 1st repetition, 02 = 2nd repetition).\n",
    "Actor               (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\"\"\"\n",
    "\n",
    "paths=[]\n",
    "emotion=[]\n",
    "soundwaves=[]\n",
    "# iterate over the files in the directory, grouped by three\n",
    "for root,dirs,files in os.walk(audio_dir):\n",
    "    for file in files:\n",
    "\n",
    "        # we only want the files with strong emotions\n",
    "        if file.split(\"-\")[3]!=\"02\":\n",
    "            continue\n",
    "\n",
    "        paths.append(file)\n",
    "        if file.split('-')[2]=='08':\n",
    "            emotion.append('07')\n",
    "        else:\n",
    "            emotion.append(file.split('-')[2])\n",
    "        y, sr = librosa.load(f\"{root}/{file}\", sr=22050,mono=True)\n",
    "        soundwaves.append(y)\n",
    "print(\"data extracted\")\n",
    "librosa.display.waveshow(soundwaves[2])\n",
    "paths = np.array(paths)\n",
    "emotion = np.array(emotion)\n",
    "soundwaves = np.array(soundwaves)\n",
    "print(\"finished !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates MEL-spectogramms for each elements of padded_soundwaves\n",
    "n_mfcc = 40\n",
    "sr = 22050\n",
    "mfcc_array = []\n",
    "\n",
    "# Generate MFCCs for each padded soundwave\n",
    "for soundwave in soundwaves:\n",
    "    # Compute MFCCs\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=soundwave, sr=sr, n_mfcc=n_mfcc).T,axis=0) \n",
    "    mfcc_array.append(mfccs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save labels as unicode characters (useful if we're later using non-latin characters)\n",
    "\n",
    "# Save the array to a .npy file\n",
    "np.save('processing_dataset/labels.npy', emotion)\n",
    "print(\"labels saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data in - final_dataset/\n",
    "import numpy as np\n",
    "\n",
    "mfccs = np.array(mfcc_array)\n",
    "np.save(\"final_dataset/mfccs.npy\", mfccs)\n",
    "print(\"mfccs saved!\")  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
